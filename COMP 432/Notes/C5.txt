Neural Networks part 2

    backpropagation issues:
        - vanishing gradient (often): gradient gets smaller and smaller (flat area)
        - exploding gradient (rare): gradient gets bigger and bigger (steep area)
              Solutions:
            - Gradient clipping: force gradient values to a max abs value (take smaller controlled steps)
        these problems get worse w/ increasing # of hidden layers

      
        Solutions to vanishing gradient:
            - non-saturating activation functions
                - saturation pts are when the function makes the value close to zero
                - so we avoid functions like tanh and sigmoid in hidden layers
                - therefore, we can use leaky ReLU (derivative is 1 for x > 0 and close to 0 for x < 0 | only on 1 side)
            
            - gradient shortcuts
                - shorter paths --> fewer multiplications --> larger gradient
                examples:
                    1. Residual Networks
                        - each layer models a distribution that is the difference between current and previous
                        - good with multiple layers
                    2. Skip connections
                        - direct connections between each hidden layer to the last one
                    3. Dense net
                        - each hidden layer takes input from all previous layers
                    
                    4. Normalization
                        - make input space smaller and similar
                        - scale feature to make mean zero and variance 1
                        Xnorm = Xi - U / o 
                            where U = mean and o = std
                        1. Batch Normalization 
                            - performed on the features found in the hidden layers
                            - speeds up training
                            - makes parameter space friendly and easier to optimize
                            - solves the issue where distribution of each layer's inputs changes during training
                            - we will still have a mean and variance to work with
                            - usually applied before linearity
                            - large learning rates possible
                            - makes training less sensitive to weight init
                            - complete running avgs during training and we use them at inference time
                        
                        2. Layer Normalization
                            - normalize over the neuron axis (horizontal) vs minibatch (vertical)
                            - just like batch norm, we can add additional learnable parameters to allow diff mean than 0 and std of 1

    Convolution Neural Networks
        - neural network that use convolution instead of linear transformation
        - differences: 
            - local connections: captures local patterns, helps determine diff patterns
            - weight saturating    
        
            we need to add padding on the edges of input:
                - # depends on filter/kernel size
                total: 2p = Ks - 1 where Ks = kernel size (usually # of weights)
                see C5_convolutionOutput for output formula
                TLDR: xi * weight i + ... xn * weight n
            
        - for each local pattern, we have a different pattern
        usually multi-channel outputs are generated by multi-channel inputs

        Stride factor:
            - hyperparameter 
            - quantifies step size
            - determines how many weights we use to calculate 1 output

        Dilated convolution:
            - insert holes between consecutive elements
            TLDR: skip some inputs to calculate (usually we take inputs near each other, now we can have a step size of inputs)
            ex: 1 2 3 4 5 6 | if stride = 3 and dilation = 2, we take the sum of 1 * w1 + 3 * w2 + 5 * w3 

        We stack multiple convolutional layers, then optionally perform normalziation followed by non-linearity
            - we get a feature maps, where we have on per output channel
            - dimesion of output: input length x output channels
    
        Receptive field
            - region input space of the particular unit of the network
            - depends on:
                - kernel size
                - # of layers
                - stride factors
                - dilation factor
            see C5_receptiveField
        
        Note:
            - we have much fewer parameters than other networks
            - # of params depend on kernel size, input channels, and output channels
            multiple kernel size by Cin or Cout if they have more than 1

    
    2D convolutions
        - # of params = kernel size sx * kernel size sy * Cin * Cout

    Pooling:
        - apply after non-linearity
        - if we care more about if a pattern is preset